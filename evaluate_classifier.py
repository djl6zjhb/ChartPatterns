import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    average_precision_score,
    brier_score_loss,
)

from walk_forward_split import WalkForwardSplit

def evaluate_classifier(model, X, y, times, threshold=0.5,):
    """
    Run walk-forward cross-validation and compute classification metrics to evaluate classifier performance.

    Parameters
    ----------
    model : estimator

    X : Feature matrix.

    y : Labels (0/1).

    times : column used to define chronological order (prevent data leakage)

    threshold : float, optional (default=0.5)

    Returns
    -------
    results : dictionary of classifer evaluation results
    """
    X = np.asarray(X) if not hasattr(X, "iloc") else X
    y = np.asarray(y)
    times = np.asarray(times)

    splitter = WalkForwardSplit(n_splits=5)

    fold_metrics = []
    y_true_all = []
    probs_all = []
    preds_all = []
    fold_ids = []

    # This chunk of code was generated by ChatGPT as it is related to the train-test splitting logic
    for fold_idx, (train_idx, test_idx) in enumerate(
        splitter.split(X, y=y, times=times),
        start=1,):

        # Slice
        if hasattr(X, "iloc"):
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
        else:
            X_train, X_test = X[train_idx], X[test_idx]

        y_train, y_test = y[train_idx], y[test_idx]
        # End ChatGPT-generated code chunk

        # Fit model
        model.fit(X_train, y_train)

        # Predict probabilities for positive class
        probs = model.predict_proba(X_test)[:, 1]
        preds = (probs >= threshold).astype(int)

        # Compute metrics for this fold
        fold_result = {}

        fold_result["accuracy"] = accuracy_score(y_test, preds)
        fold_result["precision"] = precision_score(y_test, preds, zero_division=0)
        fold_result["recall"] = recall_score(y_test, preds, zero_division=0)       
        fold_result["f1"] = f1_score(y_test, preds, zero_division=0)
        fold_result["roc_auc"] = roc_auc_score(y_test, probs)
        fold_result["pr_auc"] = average_precision_score(y_test, probs)
        fold_result["brier"] = brier_score_loss(y_test, probs)

        fold_metrics.append(fold_result)

        # Storing for global analysis
        y_true_all.append(y_test)
        probs_all.append(probs)
        preds_all.append(preds)
        fold_ids.append(np.full_like(y_test, fill_value=fold_idx, dtype=int))

        print(
            f"Fold {fold_idx}: "
            f"acc={fold_result['accuracy']:.3f}, "
            f"roc_auc={fold_result['roc_auc']:.3f}, "
            f"pr_auc={fold_result['pr_auc']:.3f}"
        )

    # Concatenate
    y_true_all = np.concatenate(y_true_all)
    probs_all = np.concatenate(probs_all)
    preds_all = np.concatenate(preds_all)
    fold_ids = np.concatenate(fold_ids)

    # Mean metrics (ignoring NaNs)
    mean_metrics = {
        metric: float(np.nanmean([m[metric] for m in fold_metrics]))
        for metric in fold_metrics[0].keys()
    }

    print("\nMean metrics over folds:")
    for k, v in mean_metrics.items():
        print(f"  {k}: {v:.4f}")

    return {
        "fold_metrics": fold_metrics,
        "mean_metrics": mean_metrics,
        "y_true_all": y_true_all,
        "probs_all": probs_all,
        "preds_all": preds_all,
        "fold_ids": fold_ids,
    }
