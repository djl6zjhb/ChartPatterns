{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "deb8d354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import yfinance as yf\n",
    "import mplfinance as mpf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37d21241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (0.2.66)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (2.3.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (2.32.5)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (4.5.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (2.4.7)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (3.18.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (4.14.2)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (6.33.1)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.11.12)\n",
      "Requirement already satisfied: pycparser in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from requests>=2.31->yfinance) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting mplfinance\n",
      "  Using cached mplfinance-0.12.10b0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from mplfinance) (3.10.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from mplfinance) (2.3.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from matplotlib->mplfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mplfinance) (1.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from pandas->mplfinance) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\daniellagalante\\documents\\chartpatterns\\.venv\\lib\\site-packages (from pandas->mplfinance) (2025.2)\n",
      "Using cached mplfinance-0.12.10b0-py3-none-any.whl (75 kB)\n",
      "Installing collected packages: mplfinance\n",
      "Successfully installed mplfinance-0.12.10b0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install yfinance\n",
    "%pip install mplfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_local_extrema(df, window=3):\n",
    "    \"\"\"\n",
    "    Very simple local high/low markers using a rolling window.\n",
    "    A point is a local high if its High is the max over [t-window, t+window].\n",
    "    Similar for local low.\n",
    "    \"\"\"\n",
    "    highs = df['High']\n",
    "    lows = df['Low']\n",
    "\n",
    "    local_high = (highs == highs.rolling(window*2+1, center=True).max())\n",
    "    local_low = (lows == lows.rolling(window*2+1, center=True).min())\n",
    "\n",
    "    return local_high.fillna(False), local_low.fillna(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "26190bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_double_tops(\n",
    "    df,\n",
    "    peak_window=3,\n",
    "    peak_tolerance=0.01,     # peaks must be within 1%\n",
    "    min_peak_gap=15,\n",
    "    max_peak_gap=30,\n",
    "    min_trough_drop=0.03,    # trough must be at least 3% below peaks\n",
    "    require_lower_second_vol=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect double top patterns and return a DataFrame of pattern events\n",
    "    *before confirmation* (confirmation checked separately).\n",
    "    \"\"\"\n",
    "    local_high, local_low = find_local_extrema(df, window=peak_window)\n",
    "    \n",
    "    peaks = df[local_high].copy()\n",
    "    troughs = df[local_low].copy()\n",
    "\n",
    "    peaks_idx = peaks.index\n",
    "    events = []\n",
    "\n",
    "    for i in range(len(peaks_idx)):\n",
    "        t1 = peaks_idx[i]\n",
    "        p1 = peaks.loc[t1]\n",
    "\n",
    "        # look for second peak within [min_peak_gap, max_peak_gap] days\n",
    "        min_date = t1 + min_peak_gap\n",
    "        max_date = t1 + max_peak_gap\n",
    "\n",
    "        candidate_peaks2 = peaks[(peaks.index >= min_date) & (peaks.index <= max_date)]\n",
    "        if candidate_peaks2.empty:\n",
    "            continue\n",
    "\n",
    "        for t2, p2 in candidate_peaks2.iterrows():\n",
    "            price1 = p1['High']\n",
    "            price2 = p2['High']\n",
    "\n",
    "            # peaks similar in height\n",
    "            if abs(price2 - price1) / price1 > peak_tolerance:\n",
    "                continue\n",
    "\n",
    "            # trough between them\n",
    "            mid_troughs = troughs[(troughs.index > t1) & (troughs.index < t2)]\n",
    "            if mid_troughs.empty:\n",
    "                continue\n",
    "\n",
    "            trough_date = mid_troughs['Low'].idxmin()\n",
    "            trough_price = mid_troughs.loc[trough_date, 'Low']\n",
    "\n",
    "            # trough must be meaningfully below peaks\n",
    "            avg_peak = (price1 + price2) / 2\n",
    "            if (avg_peak - trough_price) / avg_peak < min_trough_drop:\n",
    "                continue\n",
    "\n",
    "            # volume condition (second peak volume < first)\n",
    "            vol1 = p1['Volume']\n",
    "            vol2 = p2['Volume']\n",
    "            if require_lower_second_vol and not (vol2 < vol1):\n",
    "                continue\n",
    "\n",
    "            events.append({\n",
    "                'peak1_date': t1,\n",
    "                'peak2_date': t2,\n",
    "                'trough_date': trough_date,\n",
    "                'peak1_price': price1,\n",
    "                'peak2_price': price2,\n",
    "                'trough_price': trough_price,\n",
    "                'peak_gap_days': (t2 - t1),\n",
    "                'vol1': vol1,\n",
    "                'vol2': vol2,\n",
    "                'vol2_vol1_ratio': vol2 / vol1 if vol1 > 0 else np.nan\n",
    "            })\n",
    "\n",
    "    events_df = pd.DataFrame(events)\n",
    "    return events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "996912eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confirm_double_tops(df, events_df, max_confirm_days=20):\n",
    "    \"\"\"\n",
    "    For each candidate double top, look for first close below trough (neckline)\n",
    "    after the second peak. That date is confirmation_date.\n",
    "    \"\"\"\n",
    "    confirmed = []\n",
    "\n",
    "    for _, row in events_df.iterrows():\n",
    "        t2 = row['peak2_date']\n",
    "        neck_price = row['trough_price']\n",
    "\n",
    "        start = t2 + 1\n",
    "        end = t2 + max_confirm_days\n",
    "\n",
    "        post = df.loc[(df.index >= start) & (df.index <= end)]\n",
    "\n",
    "        below = post[post['Close'] < neck_price]\n",
    "        if below.empty:\n",
    "            continue\n",
    "\n",
    "        confirm_date = below.index[0]\n",
    "        confirm_price = below.loc[confirm_date, 'Close']\n",
    "\n",
    "        r = row.to_dict()\n",
    "        r.update({\n",
    "            'confirm_date': confirm_date,\n",
    "            'confirm_price': confirm_price\n",
    "        })\n",
    "        confirmed.append(r)\n",
    "\n",
    "    confirmed_df = pd.DataFrame(confirmed)\n",
    "    return confirmed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "450fe7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forward_returns(df, events_df, horizons=(5, 20, 60)):\n",
    "    \"\"\"\n",
    "    For each event (by confirm_date), compute forward returns.\n",
    "    Returns are simple percentage returns: Close[t+H] / Close[confirm] - 1.\n",
    "    \"\"\"\n",
    "    events_df = events_df.copy()\n",
    "    print('in cfr')\n",
    "    print(events_df.head())\n",
    "    for h in horizons:\n",
    "        col = f'ret_{h}d'\n",
    "        vals = []\n",
    "        for _, row in events_df.iterrows():\n",
    "            t0 = row['confirm_date']\n",
    "            t_fwd = df.index[df.index.get_loc(t0) + h] if (df.index.get_loc(t0) + h) < len(df.index) else None\n",
    "            if t_fwd is None:\n",
    "                vals.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            price0 = df.loc[t0, 'Close']\n",
    "            price1 = df.loc[t_fwd, 'Close']\n",
    "            vals.append(price1 / price0 - 1.0)\n",
    "        events_df[col] = vals\n",
    "    return events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bf2f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_events(df, n_events, horizons=(5, 20, 60), seed=42, buffer=60):\n",
    "    \"\"\"\n",
    "    Sample random confirmation dates from df index, avoiding first/last `buffer` days\n",
    "    so all horizons fit. Returns a DataFrame similar to events_df with forward returns.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = df.index\n",
    "\n",
    "    valid_idx = idx[buffer:-buffer]  # avoid edges\n",
    "    chosen_idx = rng.choice(valid_idx, size=min(n_events, len(valid_idx)), replace=False)\n",
    "\n",
    "    events = []\n",
    "    for t0 in chosen_idx:\n",
    "        row = {'confirm_date': t0, 'confirm_price': df.loc[t0, 'Close']}\n",
    "        events.append(row)\n",
    "    rand_df = pd.DataFrame(events)\n",
    "\n",
    "    # reuse forward-return logic\n",
    "    rand_df = compute_forward_returns(df, rand_df, horizons=horizons)\n",
    "    rand_df['type'] = 'random'\n",
    "    return rand_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "729244ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ma_crossover_signals(df, short_window=20, long_window=50, horizons=(5, 20, 60)):\n",
    "    data = df.copy()\n",
    "    data['ma_short'] = data['Close'].rolling(short_window).mean()\n",
    "    data['ma_long'] = data['Close'].rolling(long_window).mean()\n",
    "\n",
    "    # short MA crossing from above to below long MA\n",
    "    prev = data.shift(1)\n",
    "    cond_prev = prev['ma_short'] > prev['ma_long']\n",
    "    cond_now = data['ma_short'] <= data['ma_long']\n",
    "    signals = data[cond_prev & cond_now].dropna()\n",
    "\n",
    "    events = []\n",
    "    for t0, row in signals.iterrows():\n",
    "        events.append({'confirm_date': t0, 'confirm_price': row['Close']})\n",
    "    ma_df = pd.DataFrame(events)\n",
    "\n",
    "    ma_df = compute_forward_returns(df, ma_df, horizons=horizons)\n",
    "    ma_df['type'] = 'ma_crossover'\n",
    "    return ma_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87ecdc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_returns(ret_series):\n",
    "    \"\"\"\n",
    "    ret_series: pandas Series of returns for a given horizon.\n",
    "    \"\"\"\n",
    "    ret = ret_series.dropna()\n",
    "    if len(ret) < 5:\n",
    "        return {\n",
    "            'n': len(ret),\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            't_stat': np.nan,\n",
    "            'p_value': np.nan,\n",
    "            'hit_ratio_neg': np.nan,\n",
    "            'sharpe': np.nan,\n",
    "            'cohen_d': np.nan\n",
    "        }\n",
    "\n",
    "    mean = ret.mean()\n",
    "    std = ret.std(ddof=1)\n",
    "    t_stat, p_value = stats.ttest_1samp(ret, popmean=0.0)\n",
    "    hit_ratio_neg = (ret < 0).mean()\n",
    "    sharpe = mean / std if std > 0 else np.nan\n",
    "    cohen_d = mean / std if std > 0 else np.nan  # same formula\n",
    "\n",
    "    return {\n",
    "        'n': len(ret),\n",
    "        'mean': mean,\n",
    "        'std': std,\n",
    "        't_stat': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'hit_ratio_neg': hit_ratio_neg,\n",
    "        'sharpe': sharpe,\n",
    "        'cohen_d': cohen_d\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31924d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all(events_dt, events_rand, events_ma, horizons=(5, 20, 60)):\n",
    "    \"\"\"\n",
    "    Build a summary DataFrame over horizons and event types.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for h in horizons:\n",
    "        col = f'ret_{h}d'\n",
    "        for label, df_src in [('double_top', events_dt),\n",
    "                              ('random', events_rand),\n",
    "                              ('ma_crossover', events_ma)]:\n",
    "            stats_dict = summarize_returns(df_src[col])\n",
    "            stats_dict.update({'horizon': h, 'type': label})\n",
    "            records.append(stats_dict)\n",
    "\n",
    "    summary_df = pd.DataFrame(records)\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e402a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_weekly_top(df, events_df, lookback_weeks=12):\n",
    "    \"\"\"\n",
    "    For each confirmation date, check if it's within a weekly bar that\n",
    "    is near the max weekly close over a lookback window – crude 'weekly top'.\n",
    "    \"\"\"\n",
    "    # Resample to weekly close\n",
    "    weekly = df['Close'].resample('W-FRI').last()\n",
    "    weekly_max = weekly.rolling(lookback_weeks).max()\n",
    "\n",
    "    tags = []\n",
    "    for _, row in events_df.iterrows():\n",
    "        t_conf = row['confirm_date']\n",
    "        # find the week label corresponding to this day\n",
    "        week_label = weekly.index[weekly.index.get_loc(t_conf, method='bfill')]\n",
    "        wk_close = weekly.loc[week_label]\n",
    "        wk_max = weekly_max.loc[week_label]\n",
    "        if pd.isna(wk_max):\n",
    "            tags.append(False)\n",
    "        else:\n",
    "            # consider it a weekly top if within 1% of the rolling max\n",
    "            tags.append((wk_max - wk_close) / wk_max <= 0.01)\n",
    "\n",
    "    events_df = events_df.copy()\n",
    "    events_df['is_weekly_top'] = tags\n",
    "    return events_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb82bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_return_distributions(dt_events, rand_events, horizon=20):\n",
    "    col = f'ret_{horizon}d'\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(rand_events[col].dropna(), bins=30, alpha=0.5, label='Random')\n",
    "    plt.hist(dt_events[col].dropna(), bins=30, alpha=0.5, label='Double top')\n",
    "    plt.axvline(0, linestyle='--')\n",
    "    plt.title(f'{horizon}-day return distribution')\n",
    "    plt.xlabel('Return')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2e5f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameter_heatmap(dt_events, horizon=20, gap_bins=[15,20,25,30], vol_bins=[0.0,0.5,0.8,1.0]):\n",
    "    col = f'ret_{horizon}d'\n",
    "    data = dt_events.dropna(subset=[col]).copy()\n",
    "    data['gap_bin'] = pd.cut(data['peak_gap_days'], bins=gap_bins, include_lowest=True)\n",
    "    data['vol_bin'] = pd.cut(data['vol2_vol1_ratio'], bins=vol_bins, include_lowest=True)\n",
    "\n",
    "    pivot = data.pivot_table(index='gap_bin', columns='vol_bin', values=col, aggfunc='mean')\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    im = plt.imshow(pivot.values, aspect='auto', origin='lower')\n",
    "    plt.colorbar(im, label=f'Mean {horizon}-day return')\n",
    "    plt.xticks(ticks=range(len(pivot.columns)), labels=[str(c) for c in pivot.columns], rotation=45)\n",
    "    plt.yticks(ticks=range(len(pivot.index)), labels=[str(i) for i in pivot.index])\n",
    "    plt.xlabel('vol2/vol1 bin')\n",
    "    plt.ylabel('peak_gap_days bin')\n",
    "    plt.title('Double top parameter “heatmap”')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f67d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_pattern_performance(summary_df, horizon=20):\n",
    "    row = summary_df[(summary_df['type'] == 'double_top') &\n",
    "                     (summary_df['horizon'] == horizon)].iloc[0]\n",
    "    row_rand = summary_df[(summary_df['type'] == 'random') &\n",
    "                          (summary_df['horizon'] == horizon)].iloc[0]\n",
    "\n",
    "    mean = row['mean']\n",
    "    p = row['p_value']\n",
    "    hit = row['hit_ratio_neg']\n",
    "    sharpe = row['sharpe']\n",
    "    d = row['cohen_d']\n",
    "\n",
    "    mean_rand = row_rand['mean']\n",
    "    hit_rand = row_rand['hit_ratio_neg']\n",
    "\n",
    "    text = []\n",
    "\n",
    "    text.append(f\"For {horizon}-day returns after confirmed double tops (n={int(row['n'])}), \"\n",
    "                f\"the average return is {mean:.3%} with p-value {p:.3f}.\")\n",
    "    text.append(f\"The hit ratio (probability of a negative return) is {hit:.2%} \"\n",
    "                f\"vs {hit_rand:.2%} for random dates.\")\n",
    "    text.append(f\"The Sharpe ratio is {sharpe:.2f} and the effect size (Cohen's d) is {d:.2f}.\")\n",
    "    text.append(\"By the project’s working rule, the pattern 'works' at this horizon if:\\n\"\n",
    "                \"- average return < 0 and p < 0.05;\\n\"\n",
    "                \"- hit ratio > random;\\n\"\n",
    "                \"- Sharpe > 0.5 or |d| > 0.3.\")\n",
    "\n",
    "    works = (mean < 0) and (p < 0.05) and (hit > hit_rand) and ((sharpe > 0.5) or (abs(d) > 0.3))\n",
    "    text.append(f\"Based on the current sample, the double top pattern \"\n",
    "                f\"{'meets' if works else 'does NOT meet'} these criteria at {horizon} days.\")\n",
    "\n",
    "    return \"\\n\".join(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cdc07ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_double_top_pipeline(\n",
    "    ticker: str,\n",
    "    start: str = \"2015-01-01\",\n",
    "    end: str = \"2024-12-31\",\n",
    "    horizons=(5, 20, 60),\n",
    "    save_prefix: str | None = None,\n",
    "    make_plots: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    End-to-end pipeline to test Double Top performance for one ticker.\n",
    "\n",
    "    Steps:\n",
    "      1. Download daily OHLCV via yfinance.\n",
    "      2. Detect Double Tops (strict two-peak pattern with trough).\n",
    "      3. Confirm when price closes below the trough (neckline).\n",
    "      4. Compute forward returns for double-top events (5/20/60 days by default).\n",
    "      5. Build baselines: random timestamps + bearish MA crossover.\n",
    "      6. Compute summary stats & significance tests.\n",
    "      7. Optionally save CSVs & generate plots.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ticker : str\n",
    "        Ticker symbol, e.g. \"SPY\", \"AAPL\".\n",
    "    start : str\n",
    "        Start date for data (YYYY-MM-DD).\n",
    "    end : str\n",
    "        End date for data (YYYY-MM-DD).\n",
    "    horizons : tuple of int\n",
    "        Forward-return horizons in trading days.\n",
    "    save_prefix : str or None\n",
    "        If not None, will save:\n",
    "            {save_prefix}_events.csv\n",
    "            {save_prefix}_summary.csv\n",
    "    make_plots : bool\n",
    "        If True, will show a distribution plot and a simple parameter heatmap.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dt_events : pd.DataFrame\n",
    "        Double-top events with features + forward returns.\n",
    "    rand_events : pd.DataFrame\n",
    "        Random baseline events with forward returns.\n",
    "    ma_events : pd.DataFrame\n",
    "        Moving-average crossover baseline events.\n",
    "    summary_df : pd.DataFrame\n",
    "        Summary statistics across horizons & event types.\n",
    "    text_summary_20d : str\n",
    "        Plain-language summary at the 20-day horizon (if available).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) pull data; do in full run, not local data\n",
    "    # raw = yf.download(ticker, start=start, end=end)\n",
    "    # if raw.empty:\n",
    "    #     raise ValueError(f\"No data returned for {ticker} between {start} and {end}.\")\n",
    "\n",
    "    # df = raw[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].copy().sort_index()\n",
    "\n",
    "    # 1) pull data from local files\n",
    "    df = pd.read_csv(f\"sp500/sp500/{ticker}.csv\", \n",
    "                     header=0, \n",
    "                     skiprows=[1],\n",
    "                     dtype={\n",
    "                        \"Open\": float,\n",
    "                        \"High\": float,\n",
    "                        \"Low\": float,\n",
    "                        \"Close\": float,\n",
    "                        \"Volume\": float\n",
    "                        }\n",
    "    )\n",
    "\n",
    "    # 2) detect + confirm double tops\n",
    "    dt_candidates = detect_double_tops(df, peak_window=3, peak_tolerance=0.01,min_peak_gap=15, max_peak_gap=30, min_trough_drop=0.03, require_lower_second_vol=True)\n",
    "\n",
    "    dt_confirmed = confirm_double_tops(df, dt_candidates, max_confirm_days=40)\n",
    "\n",
    "    if dt_confirmed.empty:\n",
    "        print(f\"[{ticker}] No confirmed double tops found. Consider loosening parameters.\")\n",
    "        # still build empty containers to keep interface consistent\n",
    "        dt_events = pd.DataFrame()\n",
    "        rand_events = pd.DataFrame()\n",
    "        ma_events = pd.DataFrame()\n",
    "        summary_df = pd.DataFrame()\n",
    "        return dt_events, rand_events, ma_events, summary_df, \"\"\n",
    "\n",
    "    # optional weekly-level top tagging; not implemented correctly yet\n",
    "    # dt_confirmed = tag_weekly_top(df, dt_confirmed)\n",
    "\n",
    "    # 3) forward returns for double-top events\n",
    "    dt_events = compute_forward_returns(df, dt_confirmed, horizons=horizons)\n",
    "    dt_events[\"symbol\"] = ticker\n",
    "    dt_events[\"type\"] = \"double_top\"\n",
    "\n",
    "    print(dt_events.head())\n",
    "\n",
    "\n",
    "    # 4) baselines\n",
    "    rand_events = sample_random_events(df, n_events=len(dt_events), horizons=horizons)\n",
    "    rand_events[\"symbol\"] = ticker\n",
    "\n",
    "    ma_events = ma_crossover_signals(df, horizons=horizons)\n",
    "    ma_events[\"symbol\"] = ticker\n",
    "\n",
    "    # 5) summary statistics\n",
    "    summary_df = evaluate_all(dt_events, rand_events, ma_events, horizons=horizons)\n",
    "    summary_df[\"symbol\"] = ticker\n",
    "\n",
    "    # 6) CSVs (if requested)\n",
    "    if save_prefix is not None:\n",
    "        dt_events.to_csv(f\"{save_prefix}_events.csv\", index=False)\n",
    "        summary_df.to_csv(f\"{save_prefix}_summary.csv\", index=False)\n",
    "        print(f\"Saved events to {save_prefix}_events.csv and summary to {save_prefix}_summary.csv\")\n",
    "\n",
    "    # 7) plots (optional)\n",
    "    if make_plots:\n",
    "        # choose the middle horizon for prettier plots if available\n",
    "        mid_h = horizons[len(horizons) // 2]\n",
    "        plot_return_distributions(dt_events, rand_events, horizon=mid_h)\n",
    "        plot_parameter_heatmap(dt_events, horizon=mid_h)\n",
    "\n",
    "    # 8) plain-language summary at 20d if available\n",
    "    h_for_text = 20 if 20 in horizons else horizons[len(horizons) // 2]\n",
    "    try:\n",
    "        text_summary = summarize_pattern_performance(summary_df, horizon=h_for_text)\n",
    "        print(\"\\nText summary:\\n\")\n",
    "        print(text_summary)\n",
    "    except Exception as e:\n",
    "        text_summary = f\"Could not generate text summary (likely too few events). Error: {e}\"\n",
    "        print(text_summary)\n",
    "\n",
    "    return dt_events, rand_events, ma_events, summary_df, text_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0755d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in cfr\n",
      "   peak1_date  peak2_date  trough_date  peak1_price  peak2_price  \\\n",
      "0       120.0       138.0        123.0    37.250332    37.251425   \n",
      "1       230.0       249.0        239.0    39.106458    39.263570   \n",
      "2       396.0       413.0        410.0    44.900998    44.817119   \n",
      "3       396.0       417.0        410.0    44.900998    44.630730   \n",
      "4      1047.0      1070.0       1051.0    78.013037    78.449795   \n",
      "\n",
      "   trough_price  peak_gap_days       vol1       vol2  vol2_vol1_ratio  \\\n",
      "0     35.463486           18.0  5265800.0  2949400.0         0.560105   \n",
      "1     36.878925           19.0  7988900.0  1757000.0         0.219930   \n",
      "2     42.953227           17.0  1881400.0  1645100.0         0.874402   \n",
      "3     42.953227           21.0  1881400.0  1333000.0         0.708515   \n",
      "4     73.852852           23.0  3204100.0  2180100.0         0.680409   \n",
      "\n",
      "   confirm_date  confirm_price  \n",
      "0           158      35.389771  \n",
      "1           255      36.145638  \n",
      "2           425      41.825581  \n",
      "3           425      41.825581  \n",
      "4          1079      72.120300  \n",
      "   peak1_date  peak2_date  trough_date  peak1_price  peak2_price  \\\n",
      "0       120.0       138.0        123.0    37.250332    37.251425   \n",
      "1       230.0       249.0        239.0    39.106458    39.263570   \n",
      "2       396.0       413.0        410.0    44.900998    44.817119   \n",
      "3       396.0       417.0        410.0    44.900998    44.630730   \n",
      "4      1047.0      1070.0       1051.0    78.013037    78.449795   \n",
      "\n",
      "   trough_price  peak_gap_days       vol1       vol2  vol2_vol1_ratio  \\\n",
      "0     35.463486           18.0  5265800.0  2949400.0         0.560105   \n",
      "1     36.878925           19.0  7988900.0  1757000.0         0.219930   \n",
      "2     42.953227           17.0  1881400.0  1645100.0         0.874402   \n",
      "3     42.953227           21.0  1881400.0  1333000.0         0.708515   \n",
      "4     73.852852           23.0  3204100.0  2180100.0         0.680409   \n",
      "\n",
      "   confirm_date  confirm_price    ret_5d   ret_20d   ret_60d symbol  \\\n",
      "0           158      35.389771 -0.072396 -0.052084 -0.020864      A   \n",
      "1           255      36.145638 -0.035641 -0.075897  0.009580      A   \n",
      "2           425      41.825581 -0.002005  0.058327 -0.005334      A   \n",
      "3           425      41.825581 -0.002005  0.058327 -0.005334      A   \n",
      "4          1079      72.120300  0.017102 -0.081268 -0.062637      A   \n",
      "\n",
      "         type  \n",
      "0  double_top  \n",
      "1  double_top  \n",
      "2  double_top  \n",
      "3  double_top  \n",
      "4  double_top  \n",
      "in cfr\n",
      "   confirm_date  confirm_price\n",
      "0          1822     135.010757\n",
      "1          1093      73.248512\n",
      "2          1621     139.453552\n",
      "3           272      34.356895\n",
      "4          1319      69.096466\n",
      "in cfr\n",
      "   confirm_date  confirm_price\n",
      "0            96      38.905075\n",
      "1           160      33.389877\n",
      "2           264      34.505173\n",
      "3           428      41.806942\n",
      "4           749      63.692913\n",
      "\n",
      "Text summary:\n",
      "\n",
      "For 20-day returns after confirmed double tops (n=13), the average return is -3.139% with p-value 0.085.\n",
      "The hit ratio (probability of a negative return) is 69.23% vs 30.77% for random dates.\n",
      "The Sharpe ratio is -0.52 and the effect size (Cohen's d) is -0.52.\n",
      "By the project’s working rule, the pattern 'works' at this horizon if:\n",
      "- average return < 0 and p < 0.05;\n",
      "- hit ratio > random;\n",
      "- Sharpe > 0.5 or |d| > 0.3.\n",
      "Based on the current sample, the double top pattern does NOT meet these criteria at 20 days.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_path = './sp500/sp500/'\n",
    "\n",
    "dt_events, rand_events, ma_events, summary_df, text_summary = None, None, None, None, None\n",
    "\n",
    "for current_dir, subdirs, files in os.walk(root_path):\n",
    "    for fname in files:\n",
    "        dt_events, rand_events, ma_events, summary_df, text_summary = run_double_top_pipeline(fname.split('.')[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7a338a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_candles(\n",
    "    df,\n",
    "    dt_events,\n",
    "    title=\"Double Top Candlestick Chart\",\n",
    "    volume=True,\n",
    "    mav=None,\n",
    "    date_range=None,\n",
    "    save_path=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a candlestick chart and overlay Double Top peaks, troughs, and confirmations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        OHLCV DataFrame with DatetimeIndex and columns:\n",
    "        ['Open', 'High', 'Low', 'Close', 'Volume'].\n",
    "\n",
    "    dt_events : pd.DataFrame\n",
    "        Events DataFrame with at least:\n",
    "        - 'peak1_date'\n",
    "        - 'peak2_date'\n",
    "        - 'trough_date'\n",
    "        - 'confirm_date'\n",
    "\n",
    "    title : str\n",
    "        Chart title.\n",
    "\n",
    "    volume : bool\n",
    "        Whether to include the volume subplot.\n",
    "\n",
    "    mav : tuple or None\n",
    "        Moving averages to draw, e.g. (20, 50).\n",
    "\n",
    "    date_range : tuple(str or Timestamp, str or Timestamp) or None\n",
    "        Optional (start, end) to zoom, e.g. ('2020-01-01', '2021-01-01').\n",
    "\n",
    "    save_path : str or None\n",
    "        If provided, saves the figure to this path.\n",
    "    \"\"\"\n",
    "    print(df.head())\n",
    "    print(dt_events.head())\n",
    "    # Ensure datetime types in dt_events\n",
    "    for col in [\"peak1_date\", \"peak2_date\", \"trough_date\", \"confirm_date\"]:\n",
    "        if col in dt_events.columns:\n",
    "            dt_events[col] = pd.to_datetime(dt_events[col])\n",
    "\n",
    "    # Optionally zoom to a date range\n",
    "    if date_range is not None:\n",
    "        start, end = pd.to_datetime(date_range[0]), pd.to_datetime(date_range[1])\n",
    "        df_plot = df.loc[(df.index >= start) & (df.index <= end)].copy()\n",
    "        events_plot = dt_events[\n",
    "            (dt_events[\"confirm_date\"] >= start) & (dt_events[\"confirm_date\"] <= end)\n",
    "        ].copy()\n",
    "    else:\n",
    "        df_plot = df.copy()\n",
    "        events_plot = dt_events.copy()\n",
    "\n",
    "    if df_plot.empty:\n",
    "        print(\"No data in selected date range.\")\n",
    "        return\n",
    "\n",
    "    # Initialize marker series (index aligned with df_plot)\n",
    "    idx = df_plot.index\n",
    "    peak1_series = pd.Series(np.nan, index=idx)\n",
    "    peak2_series = pd.Series(np.nan, index=idx)\n",
    "    trough_series = pd.Series(np.nan, index=idx)\n",
    "    confirm_series = pd.Series(np.nan, index=idx)\n",
    "\n",
    "    # Fill marker positions from dt_events\n",
    "    for _, row in events_plot.iterrows():\n",
    "        # Peaks plotted slightly above the high for visibility\n",
    "        for col, series in [(\"peak1_date\", peak1_series),\n",
    "                            (\"peak2_date\", peak2_series)]:\n",
    "            d = row[col]\n",
    "            if d in df_plot.index:\n",
    "                price = df_plot.loc[d, \"High\"]\n",
    "                series.loc[d] = price * 1.01  # 1% above high\n",
    "\n",
    "        # Trough plotted slightly below the low\n",
    "        d_trough = row[\"trough_date\"]\n",
    "        if d_trough in df_plot.index:\n",
    "            price = df_plot.loc[d_trough, \"Low\"]\n",
    "            trough_series.loc[d_trough] = price * 0.99  # 1% below low\n",
    "\n",
    "        # Confirmation plotted slightly above the close\n",
    "        d_conf = row[\"confirm_date\"]\n",
    "        if d_conf in df_plot.index:\n",
    "            price = df_plot.loc[d_conf, \"Close\"]\n",
    "            confirm_series.loc[d_conf] = price * 1.01  # 1% above close\n",
    "\n",
    "    # Build addplots (big, bold markers so they stand out)\n",
    "    apds = []\n",
    "\n",
    "    # Peak 1 markers (bright green ▲ with black outline)\n",
    "    if not peak1_series.isna().all():\n",
    "        apds.append(\n",
    "            mpf.make_addplot(\n",
    "                peak1_series,\n",
    "                type=\"scatter\",\n",
    "                marker=\"^\",\n",
    "                markersize=200,\n",
    "                color=\"lime\",\n",
    "                edgecolor=\"black\",\n",
    "                linewidths=1.5,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Peak 2 markers (dark green ▲)\n",
    "    if not peak2_series.isna().all():\n",
    "        apds.append(\n",
    "            mpf.make_addplot(\n",
    "                peak2_series,\n",
    "                type=\"scatter\",\n",
    "                marker=\"^\",\n",
    "                markersize=200,\n",
    "                color=\"green\",\n",
    "                edgecolor=\"black\",\n",
    "                linewidths=1.5,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Trough markers (large orange ●)\n",
    "    if not trough_series.isna().all():\n",
    "        apds.append(\n",
    "            mpf.make_addplot(\n",
    "                trough_series,\n",
    "                type=\"scatter\",\n",
    "                marker=\"o\",\n",
    "                markersize=180,\n",
    "                color=\"orange\",\n",
    "                edgecolor=\"black\",\n",
    "                linewidths=1.5,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Confirmation markers (huge red ▼ with black outline)\n",
    "    if not confirm_series.isna().all():\n",
    "        apds.append(\n",
    "            mpf.make_addplot(\n",
    "                confirm_series,\n",
    "                type=\"scatter\",\n",
    "                marker=\"v\",\n",
    "                markersize=240,\n",
    "                color=\"red\",\n",
    "                edgecolor=\"black\",\n",
    "                linewidths=1.5,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Final plot call\n",
    "    mpf.plot(\n",
    "        df_plot,\n",
    "        type=\"candle\",\n",
    "        style=\"yahoo\",\n",
    "        title=title,\n",
    "        volume=volume,\n",
    "        mav=mav,\n",
    "        addplot=apds,\n",
    "        figsize=(12, 6),\n",
    "        tight_layout=True,\n",
    "        savefig=save_path,\n",
    "        warn_too_much_data=1000000000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9e7717a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Close       High        Low       Open       Volume\n",
      "Date                                                               \n",
      "2015-01-02  24.261051  24.729274  23.821675  24.718178  212818400.0\n",
      "2015-01-05  23.577574  24.110150  23.391173  24.030263  257142000.0\n",
      "2015-01-06  23.579798  23.839428  23.218089  23.641931  263188400.0\n",
      "2015-01-07  23.910431  24.010288  23.677428  23.788382  160423600.0\n",
      "2015-01-08  24.829128  24.886824  24.121246  24.238858  237458000.0\n",
      "                     peak1_date                    peak2_date  \\\n",
      "0 1970-01-01 00:00:00.000000120 1970-01-01 00:00:00.000000138   \n",
      "1 1970-01-01 00:00:00.000000230 1970-01-01 00:00:00.000000249   \n",
      "2 1970-01-01 00:00:00.000000396 1970-01-01 00:00:00.000000413   \n",
      "3 1970-01-01 00:00:00.000000396 1970-01-01 00:00:00.000000417   \n",
      "4 1970-01-01 00:00:00.000001047 1970-01-01 00:00:00.000001070   \n",
      "\n",
      "                    trough_date  peak1_price  peak2_price  trough_price  \\\n",
      "0 1970-01-01 00:00:00.000000123    37.250332    37.251425     35.463486   \n",
      "1 1970-01-01 00:00:00.000000239    39.106458    39.263570     36.878925   \n",
      "2 1970-01-01 00:00:00.000000410    44.900998    44.817119     42.953227   \n",
      "3 1970-01-01 00:00:00.000000410    44.900998    44.630730     42.953227   \n",
      "4 1970-01-01 00:00:00.000001051    78.013037    78.449795     73.852852   \n",
      "\n",
      "   peak_gap_days       vol1       vol2  vol2_vol1_ratio  \\\n",
      "0           18.0  5265800.0  2949400.0         0.560105   \n",
      "1           19.0  7988900.0  1757000.0         0.219930   \n",
      "2           17.0  1881400.0  1645100.0         0.874402   \n",
      "3           21.0  1881400.0  1333000.0         0.708515   \n",
      "4           23.0  3204100.0  2180100.0         0.680409   \n",
      "\n",
      "                   confirm_date  confirm_price    ret_5d   ret_20d   ret_60d  \\\n",
      "0 1970-01-01 00:00:00.000000158      35.389771 -0.072396 -0.052084 -0.020864   \n",
      "1 1970-01-01 00:00:00.000000255      36.145638 -0.035641 -0.075897  0.009580   \n",
      "2 1970-01-01 00:00:00.000000425      41.825581 -0.002005  0.058327 -0.005334   \n",
      "3 1970-01-01 00:00:00.000000425      41.825581 -0.002005  0.058327 -0.005334   \n",
      "4 1970-01-01 00:00:00.000001079      72.120300  0.017102 -0.081268 -0.062637   \n",
      "\n",
      "  symbol        type  \n",
      "0      A  double_top  \n",
      "1      A  double_top  \n",
      "2      A  double_top  \n",
      "3      A  double_top  \n",
      "4      A  double_top  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./sp500/sp500/AAPL.csv\", \n",
    "                 parse_dates=[\"Date\"], \n",
    "                 index_col=\"Date\",\n",
    "                 dtype={\n",
    "                    \"Open\": float,\n",
    "                    \"High\": float,\n",
    "                    \"Low\": float,\n",
    "                    \"Close\": float,\n",
    "                    \"Volume\": float\n",
    "                },\n",
    "                header = 0,\n",
    "                skiprows=[1]\n",
    "    )\n",
    "plot_candles(df, dt_events, title=\"AAPL Candles\", mav=(20,50), save_path=\"aapl_chart.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
